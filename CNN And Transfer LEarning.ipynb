{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CNN Using Scratch And Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download the dataset from the below url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries as shown below\n",
    "\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-size all the images to this\n",
    "IMAGE_SIZE = [224, 224]\n",
    "\n",
    "train_path = 'cell_images/Train'\n",
    "valid_path = 'cell_images/Test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG\n",
    "# Here we will be using imagenet weights\n",
    "\n",
    "mobilnet = VGG19(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't train existing weights\n",
    "for layer in mobilnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # useful for getting number of output classes\n",
    "folders = glob('Dataset/Train/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset/Train\\\\Parasite', 'Dataset/Train\\\\Uninfected']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our layers - you can add more if you want\n",
    "x = Flatten()(mobilnet.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(len(folders), activation='softmax')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=mobilnet.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 50178     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,074,562\n",
      "Trainable params: 50,178\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# view the structure of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 16)      208       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 112, 112, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 32)      2080      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 56, 56, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 64)        8256      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 28, 28, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 50176)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 500)               25088500  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 1002      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,100,046\n",
      "Trainable params: 25,100,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Create Model from scratch using CNN\n",
    "model=Sequential()\n",
    "model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,activation=\"relu\"))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the model what cost and optimization method to use\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Image Data Generator to import the images from the dataset\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 416 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Make sure you provide the same target size as initialied for the image size\n",
    "training_set = train_datagen.flow_from_directory('Dataset/Train',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x224f2b497c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('Dataset/Test',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91787\\AppData\\Local\\Temp\\ipykernel_12056\\4102162479.py:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  r = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 10s 727ms/step - loss: 1.1627 - accuracy: 0.5385 - val_loss: 0.7425 - val_accuracy: 0.4776\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 10s 753ms/step - loss: 0.5821 - accuracy: 0.6707 - val_loss: 0.7776 - val_accuracy: 0.6791\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 11s 792ms/step - loss: 0.5771 - accuracy: 0.6779 - val_loss: 0.8249 - val_accuracy: 0.3582\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 11s 827ms/step - loss: 0.5508 - accuracy: 0.7260 - val_loss: 0.6585 - val_accuracy: 0.6716\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 11s 854ms/step - loss: 0.4377 - accuracy: 0.7668 - val_loss: 1.2225 - val_accuracy: 0.3955\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 11s 815ms/step - loss: 0.3976 - accuracy: 0.8149 - val_loss: 1.1466 - val_accuracy: 0.4179\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 11s 858ms/step - loss: 0.4352 - accuracy: 0.7788 - val_loss: 0.7498 - val_accuracy: 0.5448\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 12s 896ms/step - loss: 0.3755 - accuracy: 0.8221 - val_loss: 1.0414 - val_accuracy: 0.4254\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 12s 930ms/step - loss: 0.3207 - accuracy: 0.8798 - val_loss: 0.8022 - val_accuracy: 0.5224\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 13s 977ms/step - loss: 0.2776 - accuracy: 0.8726 - val_loss: 0.5921 - val_accuracy: 0.6716\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 10s 777ms/step - loss: 0.2643 - accuracy: 0.8966 - val_loss: 0.5527 - val_accuracy: 0.7015\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 13s 966ms/step - loss: 0.3188 - accuracy: 0.8774 - val_loss: 0.4356 - val_accuracy: 0.8284\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 12s 896ms/step - loss: 0.2711 - accuracy: 0.9014 - val_loss: 0.7542 - val_accuracy: 0.6716\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 11s 851ms/step - loss: 0.1874 - accuracy: 0.9327 - val_loss: 0.5456 - val_accuracy: 0.7164\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 11s 848ms/step - loss: 0.1689 - accuracy: 0.9423 - val_loss: 0.8489 - val_accuracy: 0.6418\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 11s 810ms/step - loss: 0.1611 - accuracy: 0.9447 - val_loss: 0.8421 - val_accuracy: 0.6418\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 10s 798ms/step - loss: 0.1445 - accuracy: 0.9495 - val_loss: 0.6103 - val_accuracy: 0.7313\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 10s 797ms/step - loss: 0.1370 - accuracy: 0.9495 - val_loss: 0.3788 - val_accuracy: 0.8284\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 11s 842ms/step - loss: 0.1213 - accuracy: 0.9712 - val_loss: 0.3765 - val_accuracy: 0.8284\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 12s 883ms/step - loss: 0.1204 - accuracy: 0.9543 - val_loss: 0.2653 - val_accuracy: 0.8955\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 11s 849ms/step - loss: 0.1075 - accuracy: 0.9615 - val_loss: 0.3312 - val_accuracy: 0.8358\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 11s 803ms/step - loss: 0.0751 - accuracy: 0.9784 - val_loss: 0.3400 - val_accuracy: 0.8358\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 10s 776ms/step - loss: 0.0829 - accuracy: 0.9784 - val_loss: 0.4179 - val_accuracy: 0.8209\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 11s 817ms/step - loss: 0.1001 - accuracy: 0.9639 - val_loss: 0.3153 - val_accuracy: 0.8433\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 11s 824ms/step - loss: 0.0992 - accuracy: 0.9736 - val_loss: 0.3185 - val_accuracy: 0.8731\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 10s 771ms/step - loss: 0.1054 - accuracy: 0.9639 - val_loss: 0.3742 - val_accuracy: 0.8284\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 11s 838ms/step - loss: 0.0696 - accuracy: 0.9904 - val_loss: 0.2819 - val_accuracy: 0.8881\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 11s 833ms/step - loss: 0.0936 - accuracy: 0.9663 - val_loss: 0.2547 - val_accuracy: 0.8881\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 11s 826ms/step - loss: 0.0830 - accuracy: 0.9808 - val_loss: 0.2967 - val_accuracy: 0.8955\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 11s 831ms/step - loss: 0.0486 - accuracy: 0.9832 - val_loss: 0.2137 - val_accuracy: 0.9179\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 11s 802ms/step - loss: 0.1466 - accuracy: 0.9567 - val_loss: 0.2532 - val_accuracy: 0.8881\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 11s 815ms/step - loss: 0.0968 - accuracy: 0.9736 - val_loss: 0.2051 - val_accuracy: 0.9179\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 10s 786ms/step - loss: 0.0673 - accuracy: 0.9784 - val_loss: 0.2119 - val_accuracy: 0.9179\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 11s 837ms/step - loss: 0.0410 - accuracy: 0.9880 - val_loss: 0.2430 - val_accuracy: 0.9104\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 11s 816ms/step - loss: 0.0396 - accuracy: 0.9880 - val_loss: 0.2981 - val_accuracy: 0.8955\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 11s 813ms/step - loss: 0.0421 - accuracy: 0.9880 - val_loss: 0.1521 - val_accuracy: 0.9254\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 10s 791ms/step - loss: 0.0517 - accuracy: 0.9904 - val_loss: 0.2431 - val_accuracy: 0.9104\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 11s 831ms/step - loss: 0.0289 - accuracy: 0.9952 - val_loss: 0.2081 - val_accuracy: 0.9254\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 10s 791ms/step - loss: 0.0361 - accuracy: 0.9880 - val_loss: 0.2990 - val_accuracy: 0.8881\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 11s 846ms/step - loss: 0.0320 - accuracy: 0.9952 - val_loss: 0.2054 - val_accuracy: 0.9179\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 11s 801ms/step - loss: 0.0320 - accuracy: 0.9880 - val_loss: 0.2684 - val_accuracy: 0.9030\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 11s 806ms/step - loss: 0.0433 - accuracy: 0.9928 - val_loss: 0.2399 - val_accuracy: 0.9030\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 11s 833ms/step - loss: 0.0298 - accuracy: 0.9928 - val_loss: 0.2313 - val_accuracy: 0.9254\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 11s 824ms/step - loss: 0.0320 - accuracy: 0.9904 - val_loss: 0.2029 - val_accuracy: 0.9328\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 11s 820ms/step - loss: 0.0572 - accuracy: 0.9808 - val_loss: 0.2576 - val_accuracy: 0.8881\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 11s 804ms/step - loss: 0.0654 - accuracy: 0.9856 - val_loss: 0.3483 - val_accuracy: 0.8881\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 10s 789ms/step - loss: 0.0479 - accuracy: 0.9808 - val_loss: 0.3632 - val_accuracy: 0.8806\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 11s 806ms/step - loss: 0.0571 - accuracy: 0.9832 - val_loss: 0.6716 - val_accuracy: 0.8433\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 11s 815ms/step - loss: 0.0985 - accuracy: 0.9784 - val_loss: 0.2092 - val_accuracy: 0.9254\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 11s 804ms/step - loss: 0.0268 - accuracy: 0.9904 - val_loss: 0.4207 - val_accuracy: 0.8731\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "# Run the cell. It will take some time to execute\n",
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=50,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(r.history['loss'], label='train loss')\n",
    "plt.plot(r.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('LossVal_loss')\n",
    "\n",
    "# plot the accuracy\n",
    "plt.plot(r.history['accuracy'], label='train acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('AccVal_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as a h5 file\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.save('model_vgg19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 106ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49462757, 0.50537246],\n",
       "       [0.50567913, 0.4943209 ],\n",
       "       [0.50644594, 0.493554  ],\n",
       "       [0.5038358 , 0.49616414],\n",
       "       [0.50236905, 0.49763092],\n",
       "       [0.5110368 , 0.48896322],\n",
       "       [0.53577924, 0.4642208 ],\n",
       "       [0.50217366, 0.49782634],\n",
       "       [0.514105  , 0.48589507],\n",
       "       [0.49956948, 0.5004305 ],\n",
       "       [0.5074004 , 0.49259958],\n",
       "       [0.49818197, 0.50181794],\n",
       "       [0.53205365, 0.46794638],\n",
       "       [0.51939905, 0.48060098],\n",
       "       [0.5074235 , 0.4925765 ],\n",
       "       [0.49946028, 0.5005397 ],\n",
       "       [0.514697  , 0.485303  ],\n",
       "       [0.535906  , 0.46409398],\n",
       "       [0.5202152 , 0.4797848 ],\n",
       "       [0.5103505 , 0.48964944],\n",
       "       [0.5036885 , 0.49631152],\n",
       "       [0.5126742 , 0.4873258 ],\n",
       "       [0.5044738 , 0.49552616],\n",
       "       [0.51368254, 0.48631743],\n",
       "       [0.5118374 , 0.48816264],\n",
       "       [0.50788426, 0.49211568],\n",
       "       [0.48875913, 0.5112409 ],\n",
       "       [0.5087555 , 0.4912445 ],\n",
       "       [0.5106114 , 0.48938856],\n",
       "       [0.49845228, 0.50154775],\n",
       "       [0.50880176, 0.49119827],\n",
       "       [0.4950907 , 0.50490934],\n",
       "       [0.52099895, 0.4790011 ],\n",
       "       [0.51823467, 0.48176533],\n",
       "       [0.53081906, 0.4691809 ],\n",
       "       [0.502843  , 0.49715695],\n",
       "       [0.5048216 , 0.49517843],\n",
       "       [0.49250117, 0.5074988 ],\n",
       "       [0.4944184 , 0.50558156],\n",
       "       [0.5205871 , 0.47941285],\n",
       "       [0.51763344, 0.4823666 ],\n",
       "       [0.5103068 , 0.48969322],\n",
       "       [0.52595425, 0.47404575],\n",
       "       [0.50480616, 0.4951938 ],\n",
       "       [0.51007575, 0.4899242 ],\n",
       "       [0.50282586, 0.49717405],\n",
       "       [0.51033175, 0.48966825],\n",
       "       [0.4914264 , 0.5085736 ],\n",
       "       [0.5189956 , 0.48100445],\n",
       "       [0.50070494, 0.49929512],\n",
       "       [0.5004004 , 0.4995995 ],\n",
       "       [0.5079078 , 0.49209225],\n",
       "       [0.50737953, 0.49262047],\n",
       "       [0.5035964 , 0.49640355],\n",
       "       [0.5306385 , 0.46936148],\n",
       "       [0.52347744, 0.4765225 ],\n",
       "       [0.50450677, 0.49549314],\n",
       "       [0.5321604 , 0.46783963],\n",
       "       [0.50923467, 0.49076536],\n",
       "       [0.5367026 , 0.4632974 ],\n",
       "       [0.4883081 , 0.51169187],\n",
       "       [0.49444687, 0.50555307],\n",
       "       [0.5234284 , 0.4765716 ],\n",
       "       [0.51383567, 0.4861643 ],\n",
       "       [0.5174475 , 0.48255256],\n",
       "       [0.50872695, 0.49127302],\n",
       "       [0.50798494, 0.49201512],\n",
       "       [0.50330585, 0.49669418],\n",
       "       [0.5111    , 0.48889998],\n",
       "       [0.50167674, 0.49832326],\n",
       "       [0.53158987, 0.46841016],\n",
       "       [0.51418936, 0.48581064],\n",
       "       [0.5146379 , 0.48536205],\n",
       "       [0.51194626, 0.48805377],\n",
       "       [0.52150065, 0.4784993 ],\n",
       "       [0.5159692 , 0.4840308 ],\n",
       "       [0.51619077, 0.48380923],\n",
       "       [0.5150832 , 0.48491684],\n",
       "       [0.4947973 , 0.5052027 ],\n",
       "       [0.50792265, 0.49207735],\n",
       "       [0.52051693, 0.47948307],\n",
       "       [0.515014  , 0.48498595],\n",
       "       [0.5020921 , 0.49790788],\n",
       "       [0.4953442 , 0.5046558 ],\n",
       "       [0.52045214, 0.47954783],\n",
       "       [0.49658537, 0.50341463],\n",
       "       [0.524049  , 0.47595102],\n",
       "       [0.50511324, 0.4948867 ],\n",
       "       [0.50896657, 0.49103338],\n",
       "       [0.53353906, 0.46646097],\n",
       "       [0.52755356, 0.4724464 ],\n",
       "       [0.5097752 , 0.49022484],\n",
       "       [0.50787765, 0.49212238],\n",
       "       [0.51144195, 0.48855808],\n",
       "       [0.527637  , 0.47236308],\n",
       "       [0.509892  , 0.49010804],\n",
       "       [0.5157238 , 0.48427618],\n",
       "       [0.49855867, 0.50144136],\n",
       "       [0.5284551 , 0.47154498],\n",
       "       [0.5017391 , 0.49826095],\n",
       "       [0.500695  , 0.49930504],\n",
       "       [0.51288384, 0.4871161 ],\n",
       "       [0.508566  , 0.49143404],\n",
       "       [0.5209547 , 0.4790453 ],\n",
       "       [0.50541687, 0.4945832 ],\n",
       "       [0.5075658 , 0.49243423],\n",
       "       [0.5237631 , 0.4762369 ],\n",
       "       [0.502837  , 0.4971631 ],\n",
       "       [0.5112936 , 0.4887064 ],\n",
       "       [0.50291353, 0.49708647],\n",
       "       [0.51240975, 0.48759028],\n",
       "       [0.5122943 , 0.4877057 ],\n",
       "       [0.5053026 , 0.49469745],\n",
       "       [0.50443244, 0.49556756],\n",
       "       [0.5189787 , 0.48102134],\n",
       "       [0.5219011 , 0.47809896],\n",
       "       [0.4885916 , 0.51140845],\n",
       "       [0.4996838 , 0.5003162 ],\n",
       "       [0.5015327 , 0.49846742],\n",
       "       [0.5076976 , 0.4923024 ],\n",
       "       [0.51018476, 0.4898152 ],\n",
       "       [0.52693826, 0.4730617 ],\n",
       "       [0.52376246, 0.4762375 ],\n",
       "       [0.5188133 , 0.4811867 ],\n",
       "       [0.5024845 , 0.49751556],\n",
       "       [0.5008143 , 0.4991857 ],\n",
       "       [0.5076063 , 0.49239367],\n",
       "       [0.5147362 , 0.48526376],\n",
       "       [0.5174185 , 0.48258156],\n",
       "       [0.5237502 , 0.47624987],\n",
       "       [0.48977095, 0.5102291 ],\n",
       "       [0.51302505, 0.486975  ],\n",
       "       [0.5158566 , 0.48414332],\n",
       "       [0.4887801 , 0.51121986]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at model_vgg19.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_vgg19.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\Hello\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\Hello\\lib\\site-packages\\keras\\saving\\save.py:226\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    232\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    233\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at model_vgg19.h5"
     ]
    }
   ],
   "source": [
    "model=load_model('model_vgg19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=image.load_img('Dataset/Test/Uninfected/2.png',target_size=(224,224))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=image.img_to_array(img)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 25088])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "You are passing KerasTensor(type_spec=TensorSpec(shape=(None, 25088), dtype=tf.float32, name=None), name='tf.math.truediv/truediv:0', description=\"created by layer 'tf.math.truediv'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m img_data\u001b[38;5;241m=\u001b[39mpreprocess_input(x)\n\u001b[0;32m      3\u001b[0m img_data\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mexpand_dims\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\Hello\\lib\\site-packages\\numpy\\lib\\shape_base.py:591\u001b[0m, in \u001b[0;36mexpand_dims\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m    589\u001b[0m     a \u001b[38;5;241m=\u001b[39m asarray(a)\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(axis) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    594\u001b[0m     axis \u001b[38;5;241m=\u001b[39m (axis,)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Hello\\lib\\site-packages\\keras\\engine\\keras_tensor.py:283\u001b[0m, in \u001b[0;36mKerasTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are passing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an intermediate Keras symbolic \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput/output, to a TF API that does not allow registering custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdispatchers, such as `tf.cond`, `tf.function`, gradient tapes, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `tf.map_fn`. Keras Functional model construction only supports \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF API calls that *do* support dispatching, such as `tf.math.add` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `tf.reshape`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOther APIs cannot be called directly on symbolic Keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs/outputs. You can work around \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis limitation by putting the operation in a custom Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`call` and calling that layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon this symbolic input/output.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: You are passing KerasTensor(type_spec=TensorSpec(shape=(None, 25088), dtype=tf.float32, name=None), name='tf.math.truediv/truediv:0', description=\"created by layer 'tf.math.truediv'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output."
     ]
    }
   ],
   "source": [
    "x=np.expand_dims(x,axis=0)\n",
    "img_data=preprocess_input(x)\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mimg_data\u001b[49m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_data' is not defined"
     ]
    }
   ],
   "source": [
    "a=np.argmax(model.predict(img_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[43ma\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUninfected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "if(a==1):\n",
    "    print(\"Uninfected\")\n",
    "else:\n",
    "    print(\"Infected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
